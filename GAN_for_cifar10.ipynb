{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GAN_for_cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntoLg8Db4aNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q tensorflow-gpu==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xWIqEwObkfHP",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from skimage.io import imshow\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, layers, Sequential, datasets, optimizers\n",
        "from tensorflow.keras.datasets import mnist, cifar10 \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XqRIq7Tok5NF",
        "colab": {}
      },
      "source": [
        "np.__version__, tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wppvB52IK9ao",
        "colab": {}
      },
      "source": [
        "def get_datasets(source='mnist', data_size_ratio=1.):\n",
        "  dataset = datasets.mnist if source=='mnist' else datasets.cifar10\n",
        "\n",
        "  (x_train, y_train), (x_test, y_test) = dataset.load_data()\n",
        "  train_size = int(x_train.shape[0] * data_size_ratio)\n",
        "  test_size = int(x_test.shape[0] * data_size_ratio)\n",
        "  \n",
        "  x_train, y_train = x_train[:train_size], y_train[:train_size]\n",
        "  x_test, y_test = x_test[:test_size], y_test[:test_size]\n",
        "  \n",
        "  \n",
        "\n",
        "  print(\"Xdtype: {}, Ydtype: {}\".format(x_train.dtype, y_train.dtype))\n",
        "  print(\"Training: x={} y={}, Testing: x={} y={}\".format(x_train.shape,\n",
        "                                                         y_train.shape,\n",
        "                                                         x_test.shape,\n",
        "                                                         y_test.shape))\n",
        "\n",
        "  return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UPFv8fvDQHNT",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = get_datasets(source='cifar10', data_size_ratio=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6uFcLFu9QMdw",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(x_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nAuIp_3fpTCx",
        "colab": {}
      },
      "source": [
        "def showX(X):\n",
        "    int_X_reshape = X.astype(np.uint8).reshape(-1,32,32,3).swapaxes(0,1).reshape(32,-1,3)\n",
        "    display(Image.fromarray(int_X_reshape, mode='RGB'))\n",
        "\n",
        "showX(x_train[:20])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yHdtWth9TtEv"
      },
      "source": [
        "## Test central crop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a6LEQLJUTZkb",
        "colab": {}
      },
      "source": [
        "# print(x_train.shape)\n",
        "# test = tf.image.central_crop(tf.convert_to_tensor(x_train), central_fraction=0.875)\n",
        "# print(test.shape)\n",
        "# plt.figure()\n",
        "# plt.imshow(test[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gwwLlNVuTezN"
      },
      "source": [
        "## Build Tensorflow Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2vBXURKvlIis",
        "colab": {}
      },
      "source": [
        "def preprocess(x, y, crop=False):\n",
        "  x = tf.cast(x, tf.float32) / 127.5 - 1 \n",
        "  \n",
        "  if crop:\n",
        "    x = tf.image.central_crop(x, central_fraction=0.875) # 32 -> 28\n",
        "#   x = tf.expand_dims(x, axis=-1)\n",
        "\n",
        "  y = tf.one_hot(tf.squeeze(y,axis=-1), 10)\n",
        "  \n",
        "  return x, y\n",
        "\n",
        "preprocessed_data = preprocess(x_train, y_train, crop=False)\n",
        "\n",
        "BUFFER_SIZE = x_train.shape[0]\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "ds_train = tf.data.Dataset.from_tensor_slices(preprocessed_data)\n",
        "ds_train = ds_train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UIxloSGwkgCb",
        "colab": {}
      },
      "source": [
        "a_batch = next(iter(ds_train))\n",
        "a_batch[0].shape, tf.math.reduce_max(a_batch[0]), tf.math.reduce_min(a_batch[0]), a_batch[1].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3uP7_PGXfELt",
        "colab": {}
      },
      "source": [
        "# ORIGINAL generator\n",
        "LATENT_DIM = 100\n",
        "class Generator(Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = layers.Dense(8*8*256, use_bias=False, input_shape=(LATENT_DIM + 10,))\n",
        "    self.bn1 = layers.BatchNormalization()\n",
        "    # deConV layer\n",
        "    self.convtr_1 = layers.Conv2DTranspose(128, (5,5), strides=(1,1), padding=\"SAME\", use_bias=False)\n",
        "    self.bn2 = layers.BatchNormalization()\n",
        "    # deConV layer\n",
        "    self.convtr_2 = layers.Conv2DTranspose(64, (5,5), strides=(2,2), padding=\"SAME\", use_bias=False)\n",
        "    self.bn3 = layers.BatchNormalization()\n",
        "    # deConV layer\n",
        "    self.convtr_3 = layers.Conv2DTranspose(3, (5,5), strides=(2,2), padding=\"SAME\", activation=\"tanh\")\n",
        "    \n",
        "  @tf.function\n",
        "  def call(self, inputs, is_training=False):\n",
        "    x = layers.concatenate([inputs[0], inputs[1]]) # bs, latent_dim+10\n",
        "    x = self.fc1(x)\n",
        "    x = self.bn1(x, training=is_training)\n",
        "    x = tf.nn.leaky_relu(x)\n",
        "    x = tf.reshape(x, shape=[-1,8,8,256])\n",
        "    x = self.convtr_1(x) # bs, 7, 7, 3, 128\n",
        "    x = self.bn2(x, training=is_training)\n",
        "    x = tf.nn.leaky_relu(x)\n",
        "    x = self.convtr_2(x) # bs, 14, 14, 3, 64\n",
        "    x = self.bn3(x, training=is_training)\n",
        "    x = tf.nn.leaky_relu(x)\n",
        "    \n",
        "    x = self.convtr_3(x) # bs, 28, 28, 3, 1(gray_scale)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPWjcrzWJQ3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ORIGINAL Discriminator\n",
        "class Discriminator(Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = layers.Conv2D(64, (5,5), strides=(2,2), padding=\"SAME\")\n",
        "    self.drop1 = layers.Dropout(0.3)\n",
        "    self.conv2 = layers.Conv2D(128, (5,5), strides=(2,2), padding=\"SAME\")\n",
        "    self.drop2 = layers.Dropout(0.3)\n",
        "    self.flatten = layers.Flatten()\n",
        "    self.fc1 = layers.Dense(50)\n",
        "    self.bn = layers.BatchNormalization()\n",
        "    self.fc2 = layers.Dense(1)\n",
        "  @tf.function \n",
        "  def call(self, inputs, is_training=False):\n",
        "    x = self.conv1(inputs[0]) # bs, 14, 14, 64\n",
        "    x = tf.nn.leaky_relu(x)\n",
        "    x = self.drop1(x, training=is_training)\n",
        "    x = self.conv2(x) # bs, 7, 7, 128\n",
        "    x = tf.nn.leaky_relu(x)\n",
        "    x = self.drop2(x, training=is_training)\n",
        "    x = self.flatten(x)\n",
        "    # input(x.shape)\n",
        "    x = layers.concatenate([x, inputs[1]]) # bs, 7x7x128+10\n",
        "    x = self.fc1(x) # bs, 50\n",
        "    x = self.bn(x, training=is_training)\n",
        "    x = tf.nn.leaky_relu(x)\n",
        "    x = self.fc2(x) # bs, 1\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BQmIAL-osxDY",
        "colab": {}
      },
      "source": [
        "# # Define generator\n",
        "# LATENT_DIM = 100\n",
        "# class Generator(Model):\n",
        "#   def __init__(self):\n",
        "#     super().__init__()\n",
        "#     self.fc1 = layers.Dense(8*8*128, input_shape=(LATENT_DIM + 10,))\n",
        "#     self.bn = [layers.BatchNormalization(momentum=0.9) for _ in range(7)]\n",
        "#     self.leaky_relu = [layers.LeakyReLU(alpha=0.1) for _ in range(7)]\n",
        "    \n",
        "#     self.convtr = [layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding=\"SAME\") for _ in range(2)]\n",
        "#     self.conv1 = layers.Conv2D(128, (4,4), strides=(1,1), padding=\"SAME\")\n",
        "#     self.conv2 = layers.Conv2D(128, (5,5), strides=(1,1), padding=\"SAME\")\n",
        "#     self.conv3 = layers.Conv2D(128, (5,5), strides=(1,1), padding=\"SAME\")\n",
        "#     self.conv4 = layers.Conv2D(128, (5,5), strides=(1,1), padding=\"SAME\")\n",
        "#     self.conv5 = layers.Conv2D(3, (5,5), strides=(1,1), padding=\"SAME\", activation=\"tanh\")\n",
        "    \n",
        "#   @tf.function\n",
        "#   def call(self, inputs, is_training=False):\n",
        "#     x = layers.concatenate([inputs[0], inputs[1]]) # bs, latent_dim+10\n",
        "#     x = self.fc1(x)\n",
        "#     x = self.bn[0](x, training=is_training)\n",
        "#     x = self.leaky_relu[0](x)\n",
        "#     x = tf.reshape(x, shape=[-1,8,8,128])\n",
        "\n",
        "#     x = self.conv1(x)\n",
        "#     x = self.bn[1](x, training=is_training)\n",
        "#     x = self.leaky_relu[1](x)\n",
        "\n",
        "#     x = self.convtr[0](x) \n",
        "#     x = self.bn[2](x, training=is_training)\n",
        "#     x = self.leaky_relu[2](x)\n",
        "\n",
        "#     x = self.conv2(x)\n",
        "#     x = self.bn[3](x, training=is_training)\n",
        "#     x = self.leaky_relu[3](x)\n",
        "\n",
        "#     x = self.convtr[1](x) \n",
        "#     x = self.bn[4](x, training=is_training)\n",
        "#     x = self.leaky_relu[4](x)\n",
        "\n",
        "#     x = self.conv3(x)\n",
        "#     x = self.bn[5](x, training=is_training)\n",
        "#     x = self.leaky_relu[5](x)\n",
        "\n",
        "#     x = self.conv4(x)\n",
        "#     x = self.bn[6](x, training=is_training)\n",
        "#     x = self.leaky_relu[6](x)\n",
        "    \n",
        "#     x = self.conv5(x)\n",
        "    \n",
        "#     return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k1epd4DfxBIG",
        "colab": {}
      },
      "source": [
        "# DEFINE Discriminator\n",
        "# class Discriminator(Model):\n",
        "#   def __init__(self):\n",
        "#     super().__init__()\n",
        "#     self.conv1 = layers.Conv2D(128, (3,3), strides=(1,1), padding=\"SAME\")\n",
        "#     self.conv2 = layers.Conv2D(128, (4,4), strides=(2,2), padding=\"SAME\")\n",
        "#     self.conv3 = layers.Conv2D(128, (4,4), strides=(2,2), padding=\"SAME\")\n",
        "#     self.conv4 = layers.Conv2D(128, (4,4), strides=(2,2), padding=\"SAME\")\n",
        "\n",
        "#     self.bn = [layers.BatchNormalization(momentum=0.9) for _ in range(4)]\n",
        "#     self.leaky_relu = [layers.LeakyReLU(alpha=0.1) for _ in range(5)]\n",
        "\n",
        "#     self.flatten = layers.Flatten()\n",
        "#     self.fc1 = layers.Dense(512)\n",
        "#     self.fc2 = layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "#   @tf.function \n",
        "#   def call(self, inputs, is_training=False):\n",
        "#     x = self.conv1(inputs[0])\n",
        "#     x = self.bn[0](x, training=is_training)\n",
        "#     x = self.leaky_relu[0](x)\n",
        "\n",
        "#     x = self.conv2(x)\n",
        "#     x = self.bn[1](x, training=is_training)\n",
        "#     x = self.leaky_relu[1](x)\n",
        "\n",
        "#     x = self.conv3(x)\n",
        "#     x = self.bn[2](x, training=is_training)\n",
        "#     x = self.leaky_relu[2](x)\n",
        "\n",
        "#     x = self.conv4(x)\n",
        "#     x = self.bn[3](x, training=is_training)\n",
        "#     x = self.leaky_relu[3](x)\n",
        "\n",
        "#     x = self.flatten(x)\n",
        "#     x = layers.concatenate([x, inputs[1]])\n",
        "#     x = self.fc1(x) # bs, 512\n",
        "#     x = self.leaky_relu[4](x)\n",
        "#     x = self.fc2(x) # bs, 1\n",
        "    \n",
        "#     return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t3Pzzy4i1PR4",
        "colab": {}
      },
      "source": [
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "generator.build(input_shape=[(None,LATENT_DIM),(None,10)])\n",
        "discriminator.build(input_shape=[(None,32,32,3),(None,10)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "thT18Oqc1a_2",
        "colab": {}
      },
      "source": [
        "  # Define both loss function\n",
        "def generator_loss(generated_output):\n",
        "  return tf.reduce_mean(tf.losses.binary_crossentropy(tf.ones_like(generated_output), generated_output, from_logits=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rFRwru4EHUtd",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(real_output, generated_output, flip_label=True, soft_label=True):\n",
        "  if soft_label:\n",
        "    real_label = tf.random.uniform(real_output.shape, 0.8, 1.0)\n",
        "    gen_label = tf.random.uniform(generated_output.shape, 0., 0.2)\n",
        "  \n",
        "  if flip_label:\n",
        "    a = tf.random.uniform(real_label.shape, 0.0, 1.0)\n",
        "    indices = tf.where(a < 0.05)\n",
        "    updates = 1 - tf.gather_nd(real_label, indices)\n",
        "    real_label = tf.tensor_scatter_nd_update(real_label, indices, updates)\n",
        "\n",
        "    b = tf.random.uniform(real_label.shape, 0.0, 1.0)\n",
        "    indices = tf.where(a < 0.05)\n",
        "    updates = 1 - tf.gather_nd(gen_label, indices)\n",
        "    gen_label = tf.tensor_scatter_nd_update(gen_label, indices, updates)\n",
        "\n",
        "  real_loss = tf.reduce_mean(tf.losses.binary_crossentropy(real_label, real_output, from_logits=True))\n",
        "  generated_loss = tf.reduce_mean(tf.losses.binary_crossentropy(gen_label, generated_output, from_logits=True))\n",
        "\n",
        "#   total_loss = real_loss + generated_loss\n",
        "\n",
        "  return real_loss, generated_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uwIB2SLjHnhp",
        "colab": {}
      },
      "source": [
        "generator_optimizer = optimizers.Adam(lr=2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = optimizers.Adam(3e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jFAyFVAeHp09",
        "colab": {}
      },
      "source": [
        "EPOCHS = 30\n",
        "num_examples_to_generate = 20\n",
        "\n",
        "# reuse this vector\n",
        "random_vector_for_generation = tf.random.normal([num_examples_to_generate,\n",
        "                                                 LATENT_DIM])\n",
        "condition_vector_generation = tf.one_hot(list(range(10))+list(range(10)), 10)\n",
        "\n",
        "metrics = {}\n",
        "metrics['train_Gloss'] = tf.keras.metrics.Mean('generator_loss', dtype=tf.float32)\n",
        "metrics['train_Dloss_real'] = tf.keras.metrics.Mean('discriminator_loss_real', dtype=tf.float32)\n",
        "metrics['train_Dloss_gen'] = tf.keras.metrics.Mean('discriminator_loss_gen', dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzO7zB6yIQ7G",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, labels, print_loss=False):\n",
        "  noise = tf.random.normal([labels.shape[0], LATENT_DIM]) # handle remainder\n",
        "  \n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape_real, tf.GradientTape() as disc_tape_gen :\n",
        "    generated_images = generator([noise, labels], is_training=True)\n",
        "\n",
        "    real_output = discriminator([images, labels], is_training=True)\n",
        "    generated_output = discriminator([generated_images, labels], is_training=True)\n",
        "\n",
        "    gen_loss = generator_loss(generated_output)\n",
        "    disc_loss_real, disc_loss_gen = discriminator_loss(real_output, generated_output)\n",
        "    print(\"[Sample LOSS] gen:{} disc_real:{} disc_gen:{}\".format(gen_loss, disc_loss_real, disc_loss_gen))\n",
        "\n",
        "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "  gradients_of_discriminator_real = disc_tape_real.gradient(disc_loss_real, discriminator.trainable_variables)\n",
        "  gradients_of_discriminator_gen = disc_tape_gen.gradient(disc_loss_gen, discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator_real, discriminator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator_gen, discriminator.trainable_variables))\n",
        "    \n",
        "  metrics['train_Gloss'](gen_loss)\n",
        "  metrics['train_Dloss_real'](disc_loss_real)\n",
        "  metrics['train_Dloss_gen'](disc_loss_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wH-Em-6EkVao",
        "colab": {}
      },
      "source": [
        "def plot_numBar(images):\n",
        "  images = np.uint8(images*127.5+128).clip(0, 255)\n",
        "  concat_image = np.transpose(images, [1,0,2,3]).reshape((32,-1,3))\n",
        "  display(Image.fromarray(concat_image))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qMIfu2LlL69X",
        "colab": {}
      },
      "source": [
        "PATH = 'cifar10/exp1'\n",
        "train_log_dir = 'logs/{}/'.format(PATH)\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start_time = time.time()\n",
        "  for i, (images, labels ) in enumerate(ds_train):\n",
        "    print_loss=True if i == 0 else False\n",
        "    \n",
        "    train_step(images, labels, print_loss=print_loss)\n",
        "  \n",
        "  fake = generator([random_vector_for_generation, condition_vector_generation], is_training=False)\n",
        "    \n",
        "  with train_summary_writer.as_default():\n",
        "     tf.summary.scalar('GLoss', metrics['train_Gloss'].result(), step=epoch)\n",
        "     tf.summary.scalar('DLoss_real', metrics['train_Dloss_real'].result(), step=epoch)\n",
        "     tf.summary.scalar('DLoss_gen', metrics['train_Dloss_gen'].result(), step=epoch)\n",
        "  template = 'Epoch({:.2f} sec): {}, gen_loss: {}, disc_loss_for_real: {}, disc_loss_for_gen: {}'      \n",
        "  print(template.format(time.time()-start_time, epoch, metrics['train_Gloss'].result(), metrics['train_Dloss_real'].result(), metrics['train_Dloss_gen'].result()))\n",
        "  plot_numBar(fake)\n",
        "    \n",
        "  metrics['train_Gloss'].reset_states()\n",
        "  metrics['train_Dloss_real'].reset_states()\n",
        "  metrics['train_Dloss_gen'].reset_states()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oRAd-_akG2Ty",
        "colab": {}
      },
      "source": [
        "# MODEL_PATH = 'models/cifar10_GAN/alldata'\n",
        "# generator.save_weights(MODEL_PATH, save_format='tf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zdRKkkwVOvkx",
        "colab": {}
      },
      "source": [
        "# # load & use model\n",
        "# new_model = Generator()\n",
        "# new_model.build(input_shape=[(None,LATENT_DIM),(None,10)])\n",
        "# noise = tf.random.normal([BATCH_SIZE, LATENT_DIM])\n",
        "# image, label = next(iter(ds_train))\n",
        "\n",
        "# # before load weights\n",
        "# bad_images = new_model([noise, label], is_training=True)\n",
        "# plot_numBar(bad_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jMc5lk20P-hr",
        "colab": {}
      },
      "source": [
        "# new_model.load_weights(MODEL_PATH)\n",
        "\n",
        "# my_label = list([3,3,2,2,8,8,9,9,0,0])\n",
        "# noise = tf.random.normal([len(my_label), LATENT_DIM])\n",
        "# my_label = tf.one_hot(my_label, 10)\n",
        "\n",
        "# better = new_model([noise, my_label])\n",
        "\n",
        "# plot_numBar(better)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KvL8WGFkEdfk",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}